{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AXHtKFmVxEgz",
        "uNRmQH4QxXeP",
        "EOwUmC_G3elJ",
        "1S0ldnty4gvc",
        "oUqlKiP36c6H",
        "UvKG3BGqZnrx",
        "cy6tJ1yHZZRX",
        "8FU5RwRgeSfj",
        "EIPAEkqIEtBu"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dariashcherbakovaaa/Algorithms-for-massive-data/blob/main/MBA_Shcherbakova.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Market Basket Analysis on LinkedIn job skills and job links\n",
        "\n",
        "###### **Daria Shcherbakova** *(DSE student, 17487A)*"
      ],
      "metadata": {
        "id": "L6Fly5WhsrMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Settings"
      ],
      "metadata": {
        "id": "JoGUYh5St4N5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "a85JRlZ-sm8-",
        "outputId": "6638e104-06db-44ef-c442-fc068affbc6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x78c9482a7af0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://0c608429cdc8:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Algo_Aprori</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName('Algo_Aprori').getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark\n",
        "import pyspark\n",
        "type(spark)\n",
        "\n",
        "sc = spark.sparkContext\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-qtbAlBvNDIN"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.environ['KAGGLE_USERNAME'] = \"xxxxxx\"\n",
        "# os.environ['KAGGLE_KEY'] = \"xxxxxx\""
      ],
      "metadata": {
        "id": "97eajMHWtGGk"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d asaniczka/1-3m-linkedin-jobs-and-skills-2024\n",
        "!unzip 1-3m-linkedin-jobs-and-skills-2024.zip -d job_skills"
      ],
      "metadata": {
        "id": "B6Cka6RPuzop",
        "outputId": "d0d4aa01-ed48-472f-f648-8310b1ea470f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/asaniczka/1-3m-linkedin-jobs-and-skills-2024\n",
            "License(s): ODC Attribution License (ODC-By)\n",
            "1-3m-linkedin-jobs-and-skills-2024.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  1-3m-linkedin-jobs-and-skills-2024.zip\n",
            "replace job_skills/job_skills.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: job_skills/job_skills.csv  y\n",
            "y\n",
            "\n",
            "replace job_skills/job_summary.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: job_skills/job_summary.csv  y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/job_skills/job_skills.csv'\n",
        "df_skills = spark.read.csv(data_path, inferSchema=True, header=True)\n",
        "df_skills.printSchema()\n",
        "df_skills.show()"
      ],
      "metadata": {
        "id": "g9amO0zGvASt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/job_skills/linkedin_job_postings.csv'\n",
        "df_all = spark.read.csv(data_path, inferSchema=True, header=True)\n",
        "df_all.printSchema()\n",
        "df_all.show()"
      ],
      "metadata": {
        "id": "DMVFVSkjvbx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.remove('/content/1-3m-linkedin-jobs-and-skills-2024.zip')\n",
        "# os.remove('/content/job_skills/job_skills.csv')\n",
        "# os.remove('/content/job_skills/job_summary.csv')\n",
        "# os.remove('/content/job_skills/linkedin_job_postings.csv')\n",
        "# !rmdir /content/job_skills/"
      ],
      "metadata": {
        "id": "CkgdappgwPvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data pre-processing"
      ],
      "metadata": {
        "id": "AXHtKFmVxEgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Additional dataset to select actual job"
      ],
      "metadata": {
        "id": "uNRmQH4QxXeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = df_all[['job_link', 'job_title', 'search_country', 'search_position', 'job_level']]\n",
        "df_all.show(10)"
      ],
      "metadata": {
        "id": "FtVsjU1ew6eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all.describe()"
      ],
      "metadata": {
        "id": "9H0iANb8x0Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = df_all.where((df_all['job_level'] == 'Mid senior') & (df_all['search_country'] == 'United States'))\n",
        "df_all.show(10)"
      ],
      "metadata": {
        "id": "Nei6JDTsxOGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all.groupBy('job_title').count().orderBy('count', ascending=False).show()"
      ],
      "metadata": {
        "id": "OPufFsA628QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "pattern_danalyst = r'\\bdata\\sanal\\w*'\n",
        "pattern_banalyst = r'\\bbusiness\\sanal\\w*'\n",
        "#pattern_analyst = r'\\banal\\w*'\n",
        "pattern_scientist = r'\\bdata\\sscientist\\w*'\n",
        "pattern_engineer = r'\\bdata\\sengin\\w*'"
      ],
      "metadata": {
        "id": "X0SZlL3y3BR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lower, regexp_extract\n",
        "\n",
        "df_all_sampled = df_all.filter(\n",
        "    (lower(col('job_title')).rlike(pattern_danalyst)) |\n",
        "    (lower(col('job_title')).rlike(pattern_banalyst)) |\n",
        "#    (lower(col('job_title')).rlike(pattern_analyst)) |\n",
        "    (lower(col('job_title')).rlike(pattern_scientist)) |\n",
        "    (lower(col('job_title')).rlike(pattern_engineer))\n",
        ")\n",
        "print((df_all_sampled.count(), len(df_all_sampled.columns)))"
      ],
      "metadata": {
        "id": "z2H_sch93Kbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_names = ['link', 'job_title', 'country', 'serach', 'level']\n",
        "\n",
        "df_all_sampled = df_all_sampled.toDF(*new_names)\n",
        "df_all_sampled.limit(5).toPandas()"
      ],
      "metadata": {
        "id": "Qut3_1P93RDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The main dataset with links and skills"
      ],
      "metadata": {
        "id": "EOwUmC_G3elJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_names = ['link', 'skill']\n",
        "\n",
        "df_skills = df_skills.toDF(*new_names)\n",
        "df_skills.show(5)"
      ],
      "metadata": {
        "id": "X48lXj9G3bI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_skills.describe()"
      ],
      "metadata": {
        "id": "lUSrEOEW36WE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_skills = df_skills.dropna() # drop empty, NA cells\n",
        "df_skills = df_skills.dropDuplicates() # drop repeated cells\n",
        "\n",
        "df_skills.describe()"
      ],
      "metadata": {
        "id": "E4jBQOa738qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### data merging and sampling"
      ],
      "metadata": {
        "id": "1S0ldnty4gvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_skills.groupBy('skill').count().orderBy('count', ascending=False).show() # first try to check the most \"frequent\" skills in dataset"
      ],
      "metadata": {
        "id": "gWvkRfi14L5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = df_all_sampled.join(df_skills,['link'],how='inner')\n",
        "data.count()"
      ],
      "metadata": {
        "id": "P7tKl_qw4dEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.show()"
      ],
      "metadata": {
        "id": "6vmZxw_T5mGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[['link', 'skill']]\n",
        "data.show(5)"
      ],
      "metadata": {
        "id": "0u8IhCqY55y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EDA"
      ],
      "metadata": {
        "id": "oUqlKiP36c6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "N7vttbAX6XG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pd = data.toPandas()\n",
        "df_pd.shape"
      ],
      "metadata": {
        "id": "LR2lSkjJ62Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skills = df_pd['skill'].str.split(', ').explode()\n",
        "df = pd.DataFrame(skills, index=None)\n",
        "df.shape"
      ],
      "metadata": {
        "id": "AREUdM-468Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counting = df.value_counts().sort_values(ascending = 0)\n",
        "counting"
      ],
      "metadata": {
        "id": "f06ySeRH728w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counting[:15].plot.bar()"
      ],
      "metadata": {
        "id": "A54lDJLbSZVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BMA"
      ],
      "metadata": {
        "id": "q-QwRN9vSvRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data transformation into baskets and items"
      ],
      "metadata": {
        "id": "UvKG3BGqZnrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skills = data.select('skill') # don't need links\n",
        "rdd = skills.rdd # insert data in spark\n",
        "rdd = rdd.map(lambda x: x['skill']) # transformation\n",
        "rdd.take(1)"
      ],
      "metadata": {
        "id": "gPqa5M_4Sx_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "yes, it's dirty, but..."
      ],
      "metadata": {
        "id": "dI7WYAcTTm5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.getNumPartitions() # in how many \"part\" (partitions) we may \"split out data\""
      ],
      "metadata": {
        "id": "rMqne_hQS9LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baskets = rdd.map(lambda line: line.split(', ')) # 1 description = 1 basket with all skills as items\n",
        "baskets.take(1) # 1 element = list from the baskets"
      ],
      "metadata": {
        "id": "NAayCgA2UcYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baskets' EDA"
      ],
      "metadata": {
        "id": "cy6tJ1yHZZRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lenghts = baskets.map(lambda x: len(x))\n",
        "print(f\" Max number of items in the basket is: {lenghts.max()}\\n\")\n",
        "print(f\"The average number of items in the basket is: {lenghts.mean()}\\n\")\n",
        "print(f\"The total number of baskets is: {baskets.count()}\\n\")\n",
        "print(f\"The approximate total number of items is:{round(lenghts.mean() * baskets.count())}\")"
      ],
      "metadata": {
        "id": "VNzhKfH0XHMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = round(baskets.count() * 0.01) # 1% of total nymber of baskets\n",
        "s"
      ],
      "metadata": {
        "id": "lcC-BXTnb8TH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HashTable"
      ],
      "metadata": {
        "id": "8FU5RwRgeSfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hash = baskets.flatMap(lambda line: line).distinct()\n",
        "              #flat the results into a single RDD\n",
        "hash.take(5)"
      ],
      "metadata": {
        "id": "ZfMUDhHCePtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hash_index = hash.zipWithIndex().collectAsMap()"
      ],
      "metadata": {
        "id": "UbvohzEtrdxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hash.count()"
      ],
      "metadata": {
        "id": "sYo2WJmSoxTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hashing(basket):\n",
        "    return {hash_index[skill] for skill in basket}"
      ],
      "metadata": {
        "id": "i0UtuhABD4Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hashed_baskets = baskets.map(hashing)\n",
        "print(hashed_baskets.take(1))"
      ],
      "metadata": {
        "id": "C2vzXo4yjDYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A-PRIORY"
      ],
      "metadata": {
        "id": "q5zPrwlfEE43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### The first pass (count occurencies of each item)"
      ],
      "metadata": {
        "id": "EIPAEkqIEtBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_pass = hashed_baskets.flatMap(lambda basket:[(e,1) for e in basket]) \\\n",
        "                .reduceByKey(lambda x,y:x+y) \\\n",
        "                .filter(lambda x:x[1]>s)\n",
        "\n",
        "print(\"remaining singleton\", first_pass.count())\n",
        "print(\"5 random singleton\", first_pass.take(5))"
      ],
      "metadata": {
        "id": "cLnPgdR-EzA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to count all pair composed of frequent singletons\n",
        "from itertools import combinations\n",
        "\n",
        "frequent_singletons = set(first_pass.map(lambda x:x[0]).collect())\n",
        "second_pass = hashed_baskets.flatMap(lambda basket:[(e,1) for e in combinations(sorted(basket),2)]) \\\n",
        "                 .filter(lambda x: x[0][0] in frequent_singletons) \\\n",
        "                 .filter(lambda x: x[0][1] in frequent_singletons) \\\n",
        "                 .reduceByKey(lambda x,y: x+y) \\\n",
        "                 .filter(lambda x:x[1]>s)\n",
        "\n",
        "print(second_pass.count())"
      ],
      "metadata": {
        "id": "W9a2v5BjFI_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frdd = hashed_baskets.flatMap(lambda basket:[(e,1) for e in basket]) \\\n",
        "          .reduceByKey(lambda x,y:x+y) \\\n",
        "          .filter(lambda x:x[1] > s)\n",
        "\n",
        "frequent = set(first_pass.map(lambda x:(x[0],)).collect())\n",
        "\n",
        "print(f\"remaining: {len(frequent)}, frdd {frdd.take(5)}\")\n",
        "\n",
        "k = 2\n",
        "while frdd.count() != 0:\n",
        "    frdd = hashed_baskets.flatMap(lambda basket: [(x,1) for x in combinations(sorted(basket),k)])\\\n",
        "              .filter(lambda x: all([y in frequent for y in combinations(x[0],len(x[0])-1)])) \\\n",
        "              .reduceByKey(lambda x,y:x+y) \\\n",
        "              .filter(lambda x:x[1] > s)\n",
        "\n",
        "    frequent = set(frdd.map(lambda x:x[0]).collect())\n",
        "    print(k, len(frequent), frdd.take(5))\n",
        "    k += 1"
      ],
      "metadata": {
        "id": "1fguQCFcFXtu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}